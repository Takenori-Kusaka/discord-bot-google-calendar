"""åœ°åŸŸã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

import asyncio
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
from zoneinfo import ZoneInfo

import aiohttp
import yaml
from bs4 import BeautifulSoup

from ..utils.logger import get_logger

# Playwright ã¯å‹•çš„ã‚µã‚¤ãƒˆç”¨ã«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from playwright.async_api import async_playwright

    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

logger = get_logger(__name__)


@dataclass
class RegionalEvent:
    """åœ°åŸŸã‚¤ãƒ™ãƒ³ãƒˆ"""

    title: str
    date: str
    location: str
    description: str
    url: Optional[str] = None
    source: str = ""
    target_audience: Optional[str] = None  # å¯¾è±¡å¹´é½¢å±¤


@dataclass
class EventSource:
    """ã‚¤ãƒ™ãƒ³ãƒˆã‚½ãƒ¼ã‚¹è¨­å®š"""

    name: str
    url: str
    enabled: bool
    priority: int
    selectors: dict
    dynamic: bool = False  # å‹•çš„ã‚µã‚¤ãƒˆï¼ˆVue.jsç­‰ï¼‰ãƒ•ãƒ©ã‚°
    wait_time: int = 3  # å‹•çš„ã‚µã‚¤ãƒˆã®æç”»å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰


@dataclass
class ReferenceLink:
    """å‚è€ƒãƒªãƒ³ã‚¯"""

    name: str
    url: str


class EventSearchClient:
    """åœ°åŸŸã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€Google Search APIã€Perplexity APIã‚’ä½¿ç”¨ã—ã¦
    åœ°åŸŸã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’åé›†ã—ã¾ã™ã€‚
    """

    # Google Searchç”¨ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œç´¢å¯¾è±¡
    SEARCH_TARGETS = [
        "é«˜ã®åŸã‚¤ã‚ªãƒ³ ã‚¤ãƒ™ãƒ³ãƒˆ",
        "ã‘ã„ã¯ã‚“ãªãƒ—ãƒ©ã‚¶ ã‚¤ãƒ™ãƒ³ãƒˆ",
        "æœ¨æ´¥å·å¸‚ å­è‚²ã¦ ã‚¤ãƒ™ãƒ³ãƒˆ",
        "å¥ˆè‰¯å¸‚ å­ä¾› ã‚¤ãƒ™ãƒ³ãƒˆ",
    ]

    def __init__(
        self,
        google_api_key: str,
        google_search_engine_id: str,
        perplexity_api_key: Optional[str] = None,
        timezone: str = "Asia/Tokyo",
        config_path: str = "config/event_sources.yml",
    ):
        self.google_api_key = google_api_key
        self.google_search_engine_id = google_search_engine_id
        self.perplexity_api_key = perplexity_api_key
        self.timezone = ZoneInfo(timezone)
        self.config_path = Path(config_path)

        # è¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.sources: list[EventSource] = []
        self.reference_links: list[ReferenceLink] = []
        self.scraping_config: dict = {}
        self.filtering_config: dict = {}
        self._load_config()

        logger.info(
            "Event search client initialized",
            has_perplexity=bool(perplexity_api_key),
            sources_count=len(self.sources),
        )

    def _load_config(self) -> None:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.config_path.exists():
                with open(self.config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)

                # ã‚¤ãƒ™ãƒ³ãƒˆã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
                for source_data in config.get("sources", []):
                    if source_data.get("enabled", True):
                        self.sources.append(
                            EventSource(
                                name=source_data["name"],
                                url=source_data["url"],
                                enabled=source_data.get("enabled", True),
                                priority=source_data.get("priority", 5),
                                selectors=source_data.get("selectors", {}),
                                dynamic=source_data.get("dynamic", False),
                                wait_time=source_data.get("wait_time", 3),
                            )
                        )

                # å‚è€ƒãƒªãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿
                for link_data in config.get("reference_links", []):
                    self.reference_links.append(
                        ReferenceLink(
                            name=link_data["name"],
                            url=link_data["url"],
                        )
                    )

                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
                self.scraping_config = config.get("scraping", {})
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
                self.filtering_config = config.get("filtering", {})

                # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
                self.sources.sort(key=lambda x: x.priority)

                logger.info(f"Loaded {len(self.sources)} event sources from config")
            else:
                logger.warning(f"Config file not found: {self.config_path}")
        except Exception as e:
            logger.error(f"Failed to load config: {e}")

    async def search_events(self) -> list[dict]:
        """åœ°åŸŸã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œç´¢

        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        logger.info("Starting event search")

        results = []

        # 1. Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å„ã‚µã‚¤ãƒˆã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—
        scrape_results = await self._scrape_all_sources()
        results.extend(scrape_results)
        logger.info(
            f"Scraped {len(scrape_results)} results from {len(self.sources)} sources"
        )

        # 2. Google Custom Search APIã§è£œå®Œæ¤œç´¢
        async with aiohttp.ClientSession() as session:
            for target in self.SEARCH_TARGETS:
                try:
                    search_results = await self._google_search(session, target)
                    results.extend(search_results)
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(0.5)
                except Exception as e:
                    logger.warning(f"Search failed for {target}: {e}")

        logger.info(f"Found {len(results)} total results (scrape + search)")

        # 3. Perplexityã§è£œå®Œæ¤œç´¢ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.perplexity_api_key:
            try:
                perplexity_results = await self._perplexity_search()
                results.extend(perplexity_results)
            except Exception as e:
                logger.warning(f"Perplexity search failed: {e}")

        return results

    async def _scrape_all_sources(self) -> list[dict]:
        """å…¨ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        results = []
        delay = self.scraping_config.get("request_delay", 1.0)
        timeout = self.scraping_config.get("timeout", 30)
        user_agent = self.scraping_config.get(
            "user_agent",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        )

        headers = {"User-Agent": user_agent}

        async with aiohttp.ClientSession(headers=headers) as session:
            for source in self.sources:
                try:
                    scraped = await self._scrape_source(session, source, timeout)
                    results.extend(scraped)
                    await asyncio.sleep(delay)
                except Exception as e:
                    logger.warning(f"Scrape failed for {source.name}: {e}")

        return results

    async def _scrape_source(
        self,
        session: aiohttp.ClientSession,
        source: EventSource,
        timeout: int,
    ) -> list[dict]:
        """å˜ä¸€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå‹•çš„/é™çš„ã‚µã‚¤ãƒˆå¯¾å¿œï¼‰"""
        if source.dynamic:
            # å‹•çš„ã‚µã‚¤ãƒˆï¼ˆVue.jsç­‰ï¼‰ã¯Playwrightã§å‡¦ç†
            return await self._scrape_dynamic_source(source, timeout)
        else:
            # é™çš„ã‚µã‚¤ãƒˆã¯BeautifulSoupã§å‡¦ç†
            return await self._scrape_static_source(source, timeout)

    async def _scrape_static_source(
        self,
        source: EventSource,
        timeout: int,
    ) -> list[dict]:
        """é™çš„ã‚µã‚¤ãƒˆã‹ã‚‰BeautifulSoupã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        import ssl

        import certifi

        results = []

        try:
            # SSLè¨¼æ˜æ›¸ã®æ¤œè¨¼ã‚’ç·©å’Œï¼ˆä¸€éƒ¨ã‚µã‚¤ãƒˆå¯¾å¿œï¼‰
            ssl_context = ssl.create_default_context(cafile=certifi.where())
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE

            connector = aiohttp.TCPConnector(ssl=ssl_context)
            async with aiohttp.ClientSession(connector=connector) as ssl_session:
                async with ssl_session.get(
                    source.url, timeout=aiohttp.ClientTimeout(total=timeout)
                ) as response:
                    if response.status != 200:
                        logger.warning(
                            f"HTTP {response.status} for {source.name}: {source.url}"
                        )
                        return []

                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºã€å¤±æ•—æ™‚ã¯utf-8/shift_jis/euc-jpã‚’è©¦è¡Œ
                    try:
                        html = await response.text()
                    except UnicodeDecodeError:
                        raw = await response.read()
                        for encoding in ["utf-8", "shift_jis", "euc-jp", "cp932"]:
                            try:
                                html = raw.decode(encoding)
                                break
                            except UnicodeDecodeError:
                                continue
                        else:
                            html = raw.decode("utf-8", errors="ignore")

                    soup = BeautifulSoup(html, "html.parser")

                    # ã‚³ãƒ³ãƒ†ãƒŠã‚»ãƒ¬ã‚¯ã‚¿ã§ã‚¤ãƒ™ãƒ³ãƒˆè¦ç´ ã‚’å–å¾—
                    container_selector = source.selectors.get("container", "article")
                    containers = soup.select(container_selector)[:10]  # æœ€å¤§10ä»¶

                    for container in containers:
                        event = self._extract_event_from_element(container, source)
                        if event:
                            results.append(event)

                    logger.info(
                        f"Scraped {len(results)} events from {source.name} (static)"
                    )

        except asyncio.TimeoutError:
            logger.warning(f"Timeout scraping {source.name}")
        except Exception as e:
            logger.warning(f"Error scraping {source.name}: {e}")

        return results

    async def _scrape_dynamic_source(
        self,
        source: EventSource,
        timeout: int,
    ) -> list[dict]:
        """å‹•çš„ã‚µã‚¤ãƒˆï¼ˆVue.jsç­‰ï¼‰ã‹ã‚‰Playwrightã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        if not PLAYWRIGHT_AVAILABLE:
            logger.warning(
                f"Playwright not available, skipping dynamic source: {source.name}"
            )
            return []

        results = []

        try:
            async with async_playwright() as p:
                # headlessãƒ¢ãƒ¼ãƒ‰ã§Chromiumã‚’èµ·å‹•
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()

                try:
                    logger.info(f"Playwright: loading {source.name} ({source.url})")

                    # ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿
                    await page.goto(
                        source.url,
                        wait_until="load",
                        timeout=timeout * 1000,
                    )

                    # Vue.jsç­‰ã®JSãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒDOMã‚’æç”»ã™ã‚‹ã®ã‚’å¾…ã¤
                    await asyncio.sleep(source.wait_time)

                    # HTMLã‚’å–å¾—ã—ã¦BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
                    html_content = await page.content()
                    soup = BeautifulSoup(html_content, "html.parser")

                    # ã‚³ãƒ³ãƒ†ãƒŠã‚»ãƒ¬ã‚¯ã‚¿ã§ã‚¤ãƒ™ãƒ³ãƒˆè¦ç´ ã‚’å–å¾—
                    container_selector = source.selectors.get("container", "article")
                    containers = soup.select(container_selector)[:10]  # æœ€å¤§10ä»¶

                    for container in containers:
                        event = self._extract_event_from_element(container, source)
                        if event:
                            results.append(event)

                    logger.info(
                        f"Scraped {len(results)} events from {source.name} (dynamic)"
                    )

                finally:
                    await browser.close()

        except asyncio.TimeoutError:
            logger.warning(f"Timeout scraping dynamic source {source.name}")
        except Exception as e:
            logger.warning(f"Error scraping dynamic source {source.name}: {e}")

        return results

    def _extract_event_from_element(
        self, element: BeautifulSoup, source: EventSource
    ) -> Optional[dict]:
        """HTMLè¦ç´ ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º"""
        try:
            selectors = source.selectors

            # ã‚¿ã‚¤ãƒˆãƒ«
            title_elem = element.select_one(selectors.get("title", "h2, h3"))
            title = title_elem.get_text(strip=True) if title_elem else ""

            if not title:
                return None

            # æ—¥ä»˜
            date_elem = element.select_one(selectors.get("date", ".date, time"))
            date = date_elem.get_text(strip=True) if date_elem else ""

            # èª¬æ˜
            desc_elem = element.select_one(selectors.get("description", "p"))
            description = desc_elem.get_text(strip=True)[:200] if desc_elem else ""

            # ãƒªãƒ³ã‚¯
            link_elem = element.select_one(selectors.get("link", "a"))
            link = ""
            if link_elem and link_elem.get("href"):
                href = link_elem["href"]
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                if href.startswith("/"):
                    from urllib.parse import urlparse

                    parsed = urlparse(source.url)
                    link = f"{parsed.scheme}://{parsed.netloc}{href}"
                elif href.startswith("http"):
                    link = href
                else:
                    link = source.url

            return {
                "title": title,
                "snippet": f"{date} - {description}" if date else description,
                "link": link,
                "source": f"scrape:{source.name}",
                "query": source.name,
            }

        except Exception as e:
            logger.debug(f"Failed to extract event: {e}")
            return None

    async def _google_search(
        self, session: aiohttp.ClientSession, query: str
    ) -> list[dict]:
        """Google Custom Search APIã§æ¤œç´¢"""
        # ä»Šé€±æœ«ã®æ—¥ä»˜ã‚’å«ã‚ã¦ã‚ˆã‚Šå…·ä½“çš„ã«æ¤œç´¢
        now = datetime.now(self.timezone)
        weekend = now + timedelta(days=(5 - now.weekday()) % 7)
        date_str = weekend.strftime("%Yå¹´%mæœˆ")

        full_query = f"{query} {date_str}"

        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": self.google_api_key,
            "cx": self.google_search_engine_id,
            "q": full_query,
            "num": 5,
            "lr": "lang_ja",
        }

        async with session.get(url, params=params) as response:
            if response.status != 200:
                logger.warning(f"Google Search API error: {response.status}")
                return []

            data = await response.json()
            items = data.get("items", [])

            results = []
            for item in items:
                results.append(
                    {
                        "title": item.get("title", ""),
                        "snippet": item.get("snippet", ""),
                        "link": item.get("link", ""),
                        "source": "google_search",
                        "query": query,
                    }
                )

            return results

    async def _perplexity_search(self) -> list[dict]:
        """Perplexity APIã§åœ°åŸŸã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œç´¢"""
        now = datetime.now(self.timezone)
        weekend_start = now + timedelta(days=(5 - now.weekday()) % 7)
        weekend_end = weekend_start + timedelta(days=2)

        prompt = f"""
ä»Šé€±æœ«ï¼ˆ{weekend_start.strftime('%mæœˆ%dæ—¥')}ã€œ{weekend_end.strftime('%mæœˆ%dæ—¥')}ï¼‰ã«
æœ¨æ´¥å·å¸‚ã€å¥ˆè‰¯å¸‚ã€ç²¾è¯ç”ºã€é«˜ã®åŸå‘¨è¾ºã§é–‹å‚¬ã•ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ•™ãˆã¦ãã ã•ã„ã€‚

ç‰¹ã«ä»¥ä¸‹ã®æ–½è¨­ãƒ»åœ°åŸŸã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’é‡ç‚¹çš„ã«:
- é«˜ã®åŸã‚¤ã‚ªãƒ³
- ã‘ã„ã¯ã‚“ãªãƒ—ãƒ©ã‚¶
- ã‚¬ãƒ¼ãƒ‡ãƒ³ãƒ¢ãƒ¼ãƒ«æœ¨æ´¥å·
- ã‚¢ã‚¹ãƒ”ã‚¢ã‚„ã¾ã—ã‚
- æœ¨æ´¥å·å¸‚ã®å…¬å…±ã‚¤ãƒ™ãƒ³ãƒˆ

å­ä¾›ï¼ˆ0æ­³ã€œ4æ­³ï¼‰é€£ã‚Œã§å‚åŠ ã§ãã‚‹ã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚Œã°å„ªå…ˆã—ã¦æ•™ãˆã¦ãã ã•ã„ã€‚
"""

        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.perplexity_api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": "sonar",  # 2025å¹´ä»¥é™ã®æ–°ãƒ¢ãƒ‡ãƒ«å
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2,
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=payload) as response:
                if response.status != 200:
                    logger.warning(f"Perplexity API error: {response.status}")
                    return []

                data = await response.json()
                content = data["choices"][0]["message"]["content"]

                return [
                    {
                        "title": "Perplexityæ¤œç´¢çµæœ",
                        "snippet": content,
                        "link": "",
                        "source": "perplexity",
                        "query": "é€±æœ«ã‚¤ãƒ™ãƒ³ãƒˆç·åˆæ¤œç´¢",
                    }
                ]

    def get_reference_links(self) -> list[dict]:
        """å‚è€ƒãƒªãƒ³ã‚¯ã‚’å–å¾—

        Returns:
            å‚è€ƒãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ [{"name": "...", "url": "..."}]
        """
        return [{"name": link.name, "url": link.url} for link in self.reference_links]

    def format_reference_links(self) -> str:
        """å‚è€ƒãƒªãƒ³ã‚¯ã‚’é€šçŸ¥ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸå‚è€ƒãƒªãƒ³ã‚¯æ–‡å­—åˆ—
        """
        if not self.reference_links:
            return ""

        lines = ["", "ğŸ“ **ã‚‚ã£ã¨ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¢ã™**"]
        for link in self.reference_links:
            lines.append(f"â€¢ [{link.name}]({link.url})")

        return "\n".join(lines)
